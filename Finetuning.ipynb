{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11599,
     "status": "ok",
     "timestamp": 1739537437985,
     "user": {
      "displayName": "Prathamjyot Singh",
      "userId": "15862000752257668268"
     },
     "user_tz": -330
    },
    "id": "kXPfCiRJMawX",
    "outputId": "2a17ca74-e994-470c-f1a7-5e8c64c51cd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.2)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.2)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
      "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.5)\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.17)\n",
      "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.3.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.1.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.37)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.33 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.33)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.5)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.33->langchain) (3.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.2)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.0)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers peft datasets accelerate bitsandbytes torch pdfplumber langchain pypdf\n",
    "!pip install torch transformers accelerate peft datasets bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1013,
     "status": "ok",
     "timestamp": 1739537438994,
     "user": {
      "displayName": "Prathamjyot Singh",
      "userId": "15862000752257668268"
     },
     "user_tz": -330
    },
    "id": "S1hv9IE0Mg0W"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178,
     "referenced_widgets": [
      "13e69542c51d4499831050f69892f844",
      "aaa8289439254898aafe11e3a1f62d81",
      "26d36ee6dc0340c3832c6211cef78852",
      "fedc50a9530e4906a2d38656b4010eb6",
      "be0c51c2680e4e6885a218760f4e0bc8",
      "2903792780d140e9b18fbd1afa66342b",
      "b34a5f8056c347cfb59bdb22b504d6d6",
      "b8f51492142940329e57a82fa7c97fe7",
      "1ebcede07a384123aecd2ea624a1f949",
      "90d4af48c9594c5fb4e10e66cc489d88",
      "6018d371ffe649c08060b285be00acfc"
     ]
    },
    "executionInfo": {
     "elapsed": 58465,
     "status": "ok",
     "timestamp": 1739537497457,
     "user": {
      "displayName": "Prathamjyot Singh",
      "userId": "15862000752257668268"
     },
     "user_tz": -330
    },
    "id": "XZr-HoWsQTKl",
    "outputId": "7c803410-97bd-4219-b474-fefe10b87b7d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e69542c51d4499831050f69892f844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"google/gemma-2-2b-it\"\n",
    "\n",
    "# Load tokenizer & model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 783,
     "status": "ok",
     "timestamp": 1739537517983,
     "user": {
      "displayName": "Prathamjyot Singh",
      "userId": "15862000752257668268"
     },
     "user_tz": -330
    },
    "id": "_U5a0w4vTzZB",
    "outputId": "51d759ee-340c-446e-fdb7-5389def8aee7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,194,880 || all params: 2,617,536,768 || trainable%: 0.1221\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16, # Rank of low-rank matrix\n",
    "    lora_alpha=32, # Scaling factor\n",
    "    lora_dropout=0.05, # Dropout rate\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5933,
     "status": "ok",
     "timestamp": 1739537527774,
     "user": {
      "displayName": "Prathamjyot Singh",
      "userId": "15862000752257668268"
     },
     "user_tz": -330
    },
    "id": "6o6LI_aDODUR"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ccdv/arxiv-summarization\")\n",
    "dataset = dataset[\"train\"].shuffle().select(range(5000))  # Use a subset for fine-tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1486,
     "status": "ok",
     "timestamp": 1739537529258,
     "user": {
      "displayName": "Prathamjyot Singh",
      "userId": "15862000752257668268"
     },
     "user_tz": -330
    },
    "id": "x58OqcGwOHQD",
    "outputId": "82886696-b272-45b3-cbdd-cb1c966ccf39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available online at www.sciencedirect.com\n",
      "ScienceDirect\n",
      "Energy Reports 9 (2023) 247–257\n",
      "www.elsevier.com/locate/egyr\n",
      "TMREES23-Fr, EURACA 06–08 February 2023, Metz-Grand Est, France\n",
      "CNN-based, contextualized, real-time fire detection in computational\n",
      "resource-constrained environments\n",
      "Eleni\n",
      "Tsaleraa,∗\n",
      ", Andreas Papadakisb, Ioannis Voyiatzisa, Maria Samarakoua\n",
      "aDepartmentofInformaticsandComputerEngineering,UniversityofWestAttica,AgiouSpyridonos,Egaleo,12243,Greece\n",
      "bDepartmentofElectricalandElectronicsEngineeringEducators,SchoolofPedagogicalandTechnologicalEducation,Athens,14122,Greece\n",
      "Received19May2023;accepted29May2023\n",
      "Availableonline9June2023\n",
      "Abstract\n",
      "The increasing occurrence of wildfires, amplified by the changing climate conditions and drought, poses threats to human\n",
      "lives, the environment and the geographically dispersed infrastructures. Such impact necessitates the prompt identification of\n",
      "wildfires so that appropriate countermeasures are taken. The availability of electronic equipment, such as Unmanned Aerial\n",
      "Vehicles, allows for images from dynamically changing, geographical areas, which must be directly processed for wildfire\n",
      "identification and contextualization. In this work, we identify the requirements and the constraints in terms of computational\n",
      "resources of this workflow, and investigate lightweight CNNs to be used. SqueezeNet, ShuffleNet, MobileNetv2 as well as\n",
      "ResNet50 are used for fire identification. To simulate the realistic conditions, we have investigated multiple datasets, selecting\n",
      "Forest-Fire and Fire-Flame datasets and images from 3rd party sources and performed cross-dataset identification evaluation.\n",
      "To rationalize the required computational resources and the operation cost, lightweight networks have been selected and\n",
      "compared with ResNet-50, which is more complex. The contextualization, i.e. the detection of elements related to energy\n",
      "infrastructures, has been based on image semantic segmentation, performed through ResNet-18. The identification results,\n",
      "expressed as classification accuracy has reached 96%, with satisfactory results in the cross dataset scenarios, while we have\n",
      "identified five classes from the CamVid dataset which can be used for the contextualization needs.\n",
      "©2023TheAuthor(s).PublishedbyElsevierLtd.ThisisanopenaccessarticleundertheCCBY-NC-NDlicense\n",
      "(http://creativecommons.org/licenses/by-nc-nd/4.0/).\n",
      "Peer-reviewunderresponsibilityofthescientificcommitteeoftheTMREES23-Fr,EURACA,2023.\n",
      "Keywords:Firedetection;Semanticsegmentation;Environmentalprotection;Identificationandcontextualizationwildfire;Energyinfrastructures;\n",
      "CNNs\n",
      "1. Introduction\n",
      "Wildfires pose serious threat to human lives, natural environment, and the infrastructures. The climate change\n",
      "amplifies the frequency and impact of wildfires. This increasing tendency is verified by statistics, as in 2021, fires\n",
      "weremappedin22oftheEuropeanUnion27MemberStates,burning500,566hectares(ha)intotal,morethanthe\n",
      "approximate 340,000 ha of 2020 (with the exception of\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        text = \"\\n\".join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "pdf_text = extract_text_from_pdf(\"/content/1-s2.0-S2352484723010041-main (1).pdf\")\n",
    "print(pdf_text[:3000])  # Print first 1000 characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 677,
     "status": "ok",
     "timestamp": 1739537533029,
     "user": {
      "displayName": "Prathamjyot Singh",
      "userId": "15862000752257668268"
     },
     "user_tz": -330
    },
    "id": "4qNoZF2IPOCH"
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"instruction\": \"Summarize the research paper.\",\n",
    "        \"input\": pdf_text,\n",
    "        \"response\": \"This study explores lightweight CNNs such as SqueezeNet, ShuffleNet, and ResNet50 for wildfire identification and contextualization, achieving 96% classification accuracy.\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1739537533029,
     "user": {
      "displayName": "Prathamjyot Singh",
      "userId": "15862000752257668268"
     },
     "user_tz": -330
    },
    "id": "HKXPxOuzQAGe"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"train_data.json\", \"w\") as f:\n",
    "    json.dump(data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87,
     "referenced_widgets": [
      "c9eb2b6611ab4cddb4acc970fb6a574c",
      "ce0c87fef2da4947a95355e529f42a2c",
      "3d7854e3b6304b68b9fcd1e0d2094d80",
      "e4c715592cb6416c9a0b319166feaf9d",
      "a251afdf854347f2a1d3d6f7cf80dcc5",
      "84f4ce078faa498480692f99f57387bb",
      "c40eec7094af451393d672d9336d9b30",
      "b85b211493b84a3ab46175c8deb554ce",
      "32989839807349b7957272279ac43991",
      "c02f03f5a7334b6a81163e3aa1946d45",
      "99ded231f22f4e36a949868736f43631"
     ]
    },
    "executionInfo": {
     "elapsed": 692,
     "status": "ok",
     "timestamp": 1739537533718,
     "user": {
      "displayName": "Prathamjyot Singh",
      "userId": "15862000752257668268"
     },
     "user_tz": -330
    },
    "id": "rg4CIWGgQDfT",
    "outputId": "ecaf246a-7acf-4e56-9624-c9e209c51b62"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9eb2b6611ab4cddb4acc970fb6a574c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Summarize the research paper.', 'input': 'Available online at www.sciencedirect.com\\nScienceDirect\\nEnergy Reports 9 (2023) 247–257\\nwww.elsevier.com/locate/egyr\\nTMREES23-Fr, EURACA 06–08 February 2023, Metz-Grand Est, France\\nCNN-based, contextualized, real-time fire detection in computational\\nresource-constrained environments\\nEleni\\nTsaleraa,∗\\n, Andreas Papadakisb, Ioannis Voyiatzisa, Maria Samarakoua\\naDepartmentofInformaticsandComputerEngineering,UniversityofWestAttica,AgiouSpyridonos,Egaleo,12243,Greece\\nbDepartmentofElectricalandElectronicsEngineeringEducators,SchoolofPedagogicalandTechnologicalEducation,Athens,14122,Greece\\nReceived19May2023;accepted29May2023\\nAvailableonline9June2023\\nAbstract\\nThe increasing occurrence of wildfires, amplified by the changing climate conditions and drought, poses threats to human\\nlives, the environment and the geographically dispersed infrastructures. Such impact necessitates the prompt identification of\\nwildfires so that appropriate countermeasures are taken. The availability of electronic equipment, such as Unmanned Aerial\\nVehicles, allows for images from dynamically changing, geographical areas, which must be directly processed for wildfire\\nidentification and contextualization. In this work, we identify the requirements and the constraints in terms of computational\\nresources of this workflow, and investigate lightweight CNNs to be used. SqueezeNet, ShuffleNet, MobileNetv2 as well as\\nResNet50 are used for fire identification. To simulate the realistic conditions, we have investigated multiple datasets, selecting\\nForest-Fire and Fire-Flame datasets and images from 3rd party sources and performed cross-dataset identification evaluation.\\nTo rationalize the required computational resources and the operation cost, lightweight networks have been selected and\\ncompared with ResNet-50, which is more complex. The contextualization, i.e. the detection of elements related to energy\\ninfrastructures, has been based on image semantic segmentation, performed through ResNet-18. The identification results,\\nexpressed as classification accuracy has reached 96%, with satisfactory results in the cross dataset scenarios, while we have\\nidentified five classes from the CamVid dataset which can be used for the contextualization needs.\\n©2023TheAuthor(s).PublishedbyElsevierLtd.ThisisanopenaccessarticleundertheCCBY-NC-NDlicense\\n(http://creativecommons.org/licenses/by-nc-nd/4.0/).\\nPeer-reviewunderresponsibilityofthescientificcommitteeoftheTMREES23-Fr,EURACA,2023.\\nKeywords:Firedetection;Semanticsegmentation;Environmentalprotection;Identificationandcontextualizationwildfire;Energyinfrastructures;\\nCNNs\\n1. Introduction\\nWildfires pose serious threat to human lives, natural environment, and the infrastructures. The climate change\\namplifies the frequency and impact of wildfires. This increasing tendency is verified by statistics, as in 2021, fires\\nweremappedin22oftheEuropeanUnion27MemberStates,burning500,566hectares(ha)intotal,morethanthe\\napproximate 340,000 ha of 2020 (with the exception of 2017 when over 10,000 km2 had burnt) [1]. The situation\\n∗ Corresponding author.\\nE-mail address: etsalera@uniwa.gr (E. Tsalera).\\nhttps://doi.org/10.1016/j.egyr.2023.05.260\\n2352-4847/© 2023 The Author(s). Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (http:\\n//creativecommons.org/licenses/by-nc-nd/4.0/).\\nPeer-review under responsibility of the scientific committee of the TMREES23-Fr, EURACA, 2023.\\nE. Tsalera, A. Papadakis, I. Voyiatzis et al. Energy Reports 9 (2023) 247–257\\nis similar in the United States, where 100,000 wildfires occur each year with the National Interagency Fire Centre\\nreporting in 2022 64,127 fires affecting 7,343,939 acres and in 2021 54,976 fires affecting 6,814,073 acres [2].\\nEspeciallyfortheinfrastructure,includingtheelectricalpowerandenergysystems,wildfirescandisruptthebusiness\\ncontinuity of critical elements in power generation and distribution and cause impactful security incidents.\\nConsidering the increasing difficulties in the confrontation of wildfires, effort is dedicated to the early detection.\\nTowards this direction, new remote sensing approaches become available, including ground sensor -based systems,\\nmanned and unmanned aerial vehicle-based systems, and satellite-based systems. While a rich set of sensors (such\\nas temperature, smoke, infrared, gas, optical spectroscopic carbon monoxide sensor) can be effectively used for\\ndetection in (geographically) restricted environments, the need for extensive coverage creates challenges in the\\ncases of remote, open air and forest areas. To be able to make decisions in real time, the processing of the sensed\\ndata have to take place promptly so that fire is localized. In parallel context-based data related to infrastructure\\npotentially affected by the wildfire combined with risk analysis can allow for intelligent decisions. However, fire\\nandsmokedetectionisachallengingimage-processingtask,duetothehighlydynamicnatureofthesephenomena,\\ntheirirregularshapesandforms,aswellastheirasymmetricaldistributioninspace.Thisdynamicnatureisamplified\\nwhenimagesaretakenirregularlyintimefromdynamicallychanginggeographicalsettings.Inaddition,thequality\\nof the images may also vary, due to the capturing technique, the changing field of view, the employed equipment,\\nthe distance, the physical barriers and even the compression applied.\\nIn this work, we focus on the early detection and identification of fire in open air areas, endangering critical\\nenergy-related infrastructures. As the early fire detection belongs to the family of critical applications, involving\\nreal time decision making, a complete solution is designed including the identification and the contextualization\\n(achieved through semantic segmentation). We assume the availability of distributed image retrieval mechanisms\\n(such as UAV and/or satellite) which provide multiple, near real-time images, of medium or even low quality, in\\ndynamically changing geographic areas. We also assume that the sensing systems are equipped with computational\\nresources allowing the on-the-spot processing of the available visual material. From the technological perspective,\\nwe employ Convolutional Neural Networks (CNNs) for early fire detection and contextualization/ semantic\\nsegmentation. The particularities of the problem at hand involve a) the existence of a limited number of datasets\\n(especially considering that the characteristics of the datasets can orient or even define the approach to solve a\\nproblem and influence the performance of the algorithms) and b) the need to restrict the required computational\\nresources. For this we have considered a wide range of datasets, selecting two of them with the addition of images\\ncomingfrom3rdpartysources,toconfrontdifferentsettingsintermsofresolution,noiseandcompression,asthese\\nmay not be easily perceptible by the human visual system may affect the contained visual information used by the\\nMLmodules.Toenhancerobustnessandflexibility,weemploytransferlearning,cross-datasetevaluationandnoise\\ninclusion.\\n1.1. Similar work\\nVision-based systems have used a variety of equipment, mechanisms and techniques such as background\\nsubtraction in video acquired by static cameras [3], the usage of synthetic images to train CNNs [4], linear color\\nspaceconversiontodetectfirepixelsusingParticleSwarmOptimization(PSO)forproperweightsoftheconversion\\nmatrixandK-medoidsasafitnessmetric[5],fuzzylogicsystemsusingtheGaussianmembershipfunctions(GMFs)\\nfor the shape, size and motion variation of a fire from successive frames, along with the calculation of the distance\\nbetween the camera and the fire region [6], and robotic systems with unmanned aerial vehicles (UAVs) with color\\ndecision rule to extract fire-colored pixels [7,8]. Image classification/ detection in fire detection can take place\\nusing traditional methods (where the selection of the most appropriate and representative features is an open\\nand challenging issue [9]) and or neural networks. The former employs the features, such as color, texture, and\\nshape of smoke and fire. The latter involve neural networks and specifically convolutional as a class of neural\\nnetworks can process grid-based data and achieve high classification accuracy [10–12]. Zhang et al. [13] have\\npresented a deep learning-based detection system for forest fires, using a full-image and fine-grained patch fire\\nclassifier. Mahmoud and Ren [14] have presented a forest fire detection system that applies a rule-based image\\nprocessing technique and temporal variation. Muhammad et al. [15] have presented a deep CNN fire detection\\nsystem applied upon video. Avazov et al. [16] developed a fire detector that accurately detects even small sparks\\nand sounds an alarm within 8 s of a fire outbreak. A novel convolutional neural network was developed to detect\\n248\\nE. Tsalera, A. Papadakis, I. Voyiatzis et al. Energy Reports 9 (2023) 247–257\\nfire regions using an enhanced You Only Look Once (YOLO) v4network. Hu et al. [17] propose multi-oriented\\ndetection based on a value conversion-attention mechanism module and Mixed-NMS (MVMNet). Xu et al. [18]\\npresent a lightweight fire-detection algorithm, Light-YOLOv5 (You Only Look Once) achieving detection speed\\nof 91.1 fps., Zhang et al. [19] has designed a neural network architecture for forest fire detection and recognition\\nbased on Attention U-Net and SqueezeNet (ATT Squeeze U-Net), performing segmentation to extract the shape\\nof forest fire, and classification to verify the detected fire area. Wang et al. [20] perform forest fire detection\\nframework using superpixel-based suspicious fire region proposal and light-weight convolutional neural network\\nandspecificallytheExpandedSqueeze-and-ExcitationShuffleNet(ESE-ShuffleNet).TheusageofCNNsappearsas\\na common denominator of recent solutions, with the computational cost rationalization coming into play. Aspects\\nrelated to complex and dynamically changing backgrounds create challenges, especially due to the fact that the\\ndatasets typically focus on specific settings and backgrounds.\\n1.2. Structure of the paper\\nThe structure of the paper is as follows: Section 2 includes the methodological aspects, and specifically the\\nselection of the CNNs and the application of transfer learning to cover the constraints applied. It also discusses the\\nidentified datasets, their characteristics, the consideration of noise and the mechanisms to extend them. Section 3\\ndescribestheimplementation,includingthepreparationoftheCNNsthroughtransferlearningandtheclassification\\nofimagestowardsfiredetection.Section4presentsanddiscussestheresultsand,Section5includestheconclusion\\nand the future work.\\n2. Methodology\\n2.1. Selection of CNNs and transfer learning\\nCNN architecture is defined through the number of layers and their connections. The selection of CNNs is a\\nchallengingproblem,dependingonthecasestudy,therequirementsandconstraints.Inourcasethekeyrequirements\\nthe need for high classification accuracy, the generalization capabilities upon new data (due to the dynamically\\nchanging geographical context) and the constrained computational and telecommunications resources. In this view,\\nwe have selected four CNNs, namely a) SqueezeNet [21], as a simplified version of AlexNet, which achieves\\nsimilaraccuracywith50xfewerparametersinclassificationtasks,b)ShuffleNet[22],whichoperatesinconstrained\\nenvironments using techniques of pointwise group convolution and channel shuffle, c) MobileNet [23], which can\\nbeappliedtomobiledevicesusingdepth-wiseseparableconvolutionsandd)ResNet-50[24],asarepresentativeof\\nlarger CNNs. Table 1 describes the characteristics of the selected CNNs.\\nTable 1. Characteristics of the selected CNNs.\\nCNN SqueezeNet ShuffleNet MobileNet_v2 ResNet-50\\nDepth 18 50 53 50\\nParameters (millions) 1.24 1.4 3.5 25.6\\nConsidering limitations in the number and quality of labeled data, a typical bottleneck in supervised learning\\n[25], we have applied transfer learning for fire detection and consequent semantic segmentation for object\\ndetection (contextualization). The selected CNNs SqueezeNet, ShuffleNet, MobileNet and ResNet-50 are trained\\non ImageNet1 and they are retrained with the specific objective of fire identification. During retraining the training\\nloss (the negative log likelihood, NLL) and its gradients per model parameter are calculated (backpropagation) and\\nused to update the parameters with the optimizer. The retraining parameters is an open question, including the\\nnumber of layers to be retrained, the optimizer, the number of epochs and the learning rate.\\nTo make the framework adaptable to vertical application, such as the protection of energy infrastructure, we\\nneed to identify related objects such as buildings, roads, electricity transmission lines. Object detections cannot be\\ncontinuously applied, rather in case fire identification is positive. Faster R-CNN, You Only Look Once (YOLO)\\nv2-4, and Single Shot Detection (SSD) networks are focusing on semantic segmentation. We have used Deeplab\\nv3+ [26] network pretrained on the CamVid database [27] and selected a set of interesting objects.\\n1 http://www.image-net.org\\n249\\nE. Tsalera, A. Papadakis, I. Voyiatzis et al. Energy Reports 9 (2023) 247–257\\n2.2. Datasets\\nDatasets differ regarding the type and number of items they contain, the origin of the images, the bias, the\\nhomogeneity(ifforexamplethematerialhasbeencreatedforthispurposeortheyhavebeenqueriedfromexisting\\nimagedatabases),thetechnicalcharacteristics,thecurationandlicensingaspects.Ourresearchforreusabledatasets\\nrelated to wildfire identification has yielded relatively limited results, as in the existing datasets the visualization\\nof large-scale fires which is straightforward to identify and additionally it is not appropriate for training in an\\nearly stage of the fire. For example, using FLAME2 due to its specific background and clear fire visualization the\\naccuracy can be maximized. In this view, the datasets considered include the following: (a) Forest-Fire dataset\\n[28]:with3-channeledimagesanonymizedfromexistingdatabaseswith1900imagesandtwoclasses(fire,no-fire),\\n(b)Fire-Flame3 datasetof3classes(fire,smoke,neutral)and3000images(forcomparisonreasonsweexcludedthe\\n“smoke” class), (c) Unknown Images: a set of images that have not been used in the networks’ training procedure\\nand have been collected from various publicly available sources, with 100 images in each class. Table 2 presents\\nthe characteristics of the selected datasets.\\nTable 2. The datasets.\\nDataset Forest-Fire Fire-Flame Unknown Images\\nClasses Fire – No Fire Fire – No Fire Fire – No Fire\\nMeans Terrestrial and aerial Terrestrial and aerial Terrestrial and aerial\\nLocation Forest Everywhere Everywhere\\nResolution 250 × 250 pixels Not standard Not standard\\nView Front and top Front Front and top\\nNumber of images 1900 2000 200\\nDistribution among classes Uniform Uniform Uniform\\nSample images from each dataset are shown in Fig. 1.\\nFig. 1. Sample images from the three datasets. (All images are publicly available).\\n2 https://ieee-dataport.org/open-access/flame-dataset-aerial-imagery-pile-burn-detection-using-drones-uavs\\n3 https://github.com/DeepQuestAI/Fire-Smoke-Dataset\\n250\\nE. Tsalera, A. Papadakis, I. Voyiatzis et al. Energy Reports 9 (2023) 247–257\\n3. Implementation\\nBaseduponthetwoselecteddatasets(i.e.Forest-FireandFire-Flame)andusingthepre-trainedneuralnetworks\\nmentioned in Section 2.1 (i.e. SqueezeNet, ShuffleNet, MobileNet v2 and ResNet-50) we perform experiments\\nbased on transfer learning. Each set is divided into training set and testing set of 80% and 20% respectively. In the\\nfirst experiment we investigate the retraining options evaluating the classification accuracy. After determining the\\noptimalretrainingparametersandthemaximumoftheclassificationaccuracy,theimagesaresubjectedtoGaussian\\nand Salt & Pepper noise at two different levels, i.e. medium and high. This is to determine which network is more\\nrobust against noise and which noise has the most destructive effects.\\nTo approach realistic real-time scenarios, we also perform cross-dataset evaluations: the retrained networks are\\ntestedonasetofimagesthathavenottakenpartinthetrainingofthenetworks.Theout-of-training-domaindataset\\n(i.e. the “Unknown Images”) is used as the test set and contains publicly available images from other datasets as\\ndescribed in Section 2.2. In the images of the last set, fire occupies a different part of the image (including also\\nthe more challenging early stage). Also, these images are examined after being subjected to noise. In parallel, the\\nimages are analyzed with semantic segmentation by associating each pixel of the image with an object category\\nand in this way areas of interest (such as roads, buildings, people, vehicles, etc.) are detected in the images.\\nThe workflow of the procedure is described in Fig. 2.\\nFig. 2. Workflow scheme.\\n3.1. Training options\\nDetermining the values of the transfer learning hyper-parameters is related to the adjustment of the weights\\nusing back-propagation to maximize classification accuracy and minimize the loss function. We have considered\\nthe Stochastic Gradient Descent with Momentum (SGDM) [29] and the Adaptive Moment Estimation (Adam) [30]\\nwiththeformerachievingsuperiorresults.Next,weinvestigatefourdifferentvaluesofthemini-batchsize(50,100,\\n200 and 300) which is the divisor of the training set number of files, and the quotient that occurs is the number of\\niterationswhichareprocessedineachepochinordertoadjusttheweights.Themaximumnumberofepochsisthe\\nfullpassesofthetrainingset,whilevalidationpatienceisthenumberoftimesafterwhichthelossstopsdecreasing.\\nThe learning rate refers to the size of corrective steps in the procedure of updating the weights. The combinations\\nresultingfromthetwooptimizersandthefourvaluesofmini-batchsize(8innumber)wereiterativelytested,while\\nthemaximumnumberofepochswassettoaflexiblevaluewhichiscombinedwiththevalueofvalidationpatience.\\n251\\nE. Tsalera, A. Papadakis, I. Voyiatzis et al. Energy Reports 9 (2023) 247–257\\nThe learning rate was also set to a low value so that small steps of updating the weights are made which delays\\nthe training but leads to more optimal weight values.\\nSpecifically, the values of the hyper-parameters have been set as follows: Optimizer: SGDM, Mini-batch size:\\n100,MaximumEpochs:10(inallcasesthetrainingprocedurestoppedbeforethe10thepoch),ValidationPatience:\\n2, Learning Rate: 2 × 10−3. Furthermore, we shuffled the images in each epoch so that the update of the weights\\nconsiders different set of images and augmentation operations (reflections, stretches and translations) to avoid\\noverfitting.\\n3.2. Noise\\nAir and terrestrial monitoring images can be affected by noise, during capturing (e.g. due to aperture used\\nand capturing parameters such as shutter speed), the (lossy) compression and transmission. This means that fire\\nidentification may have to take place in low quality or noisy images. Types of noise include Gaussian [31] and\\nimpulse [32] noise, missing image samples, packet loss in image transmission, and tampered images. Such noise\\ncan be easily ignored by the human optical system, but it can affect the performance of CNN during classification.\\nConsidering that when the models classify data of the same quality as the training data they achieve the highest\\naccuracy, we decided to test the models on noisy images (while trained on clean ones) in order to estimate the\\nlower boundary of the classification accuracy. The images have been subjected to Gaussian and Salt & Pepper\\nnoises by adjusting their parameters to obtain the same peak signal-to-noise ratio (PSNR). We consider two noise\\nlevels,specificallymediumwithPSNRsetto15dB,andhighwithPSNRsetto10dB.Theeffectofthetwolevels\\nof noise on the images is shown in Fig. 3.\\nFig. 3. Visualization of the effect of the two noise levels on image.\\n3.3. Semantic segmentation\\nThesemanticsegmentationisanadditionalprocesswhich,incombinationwiththedetectionofthefire,supports\\nthe determination of the level of risk depending on the area where it is located. The detection of interesting objects\\ncangiveanindicationonthetypeofthearea,suchasresidential,forest,oraffectinginfrastructures(e.g.windfarm,\\nphotovoltaic or energy transmission lines). We have employed the ResNet-18 based Deeplab v3+ network (i.e. its\\ninitial weights are identical with those of ResNet-18) which is trained on the CamVid dataset. This dataset consists\\nof street-level views of images with pixel-level labels of 32 classes including building, car, pedestrian, tree etc.\\nAccording to the number of pixels that include the points of interest, the corresponding conclusions can be drawn.\\n252\\nE. Tsalera, A. Papadakis, I. Voyiatzis et al. Energy Reports 9 (2023) 247–257\\nFig. 4 shows an example of semantic segmentation on a successfully classified image. Although this image does\\nnot depict a forest fire, it was chosen to highlight the function of semantic segmentation since it contains objects\\nof interest.\\nFig. 4. Semantic segmentation in an image with fire.\\nAccording to the color bar on the side we see that the pedestrian, the building, the electricity pylon, the rode\\nand the plants are successfully detected. These classes can determine the level of immediacy of intervention and\\nalso the way of this (since there is a road).\\n4. Results and discussion\\nThe classification accuracy and the training time for each dataset and CNN are shown in Table 3. The hyper-\\nparameters are set to their optimal values and in each case, the training and the testing are performed on subsets\\nof the same dataset. All four CNNs achieve high accuracy rates exceeding 95%. The training time of each network\\ndepends on its depth and, on the number of images used in the training process. The largest (deepest) network,\\nMobileNet v2, takes the longest training time but in exchange, it yields the highest classification accuracy for\\nboth datasets. Considering that the training procedure happens once, so far MobileNet v2 is the dominant CNN.\\nRegarding the two datasets, the classification results are quite similar with slightly better results being obtained for\\nthe Forest-Fire dataset.\\nTable 3. Classification accuracy and training time per dataset and CNN with optimal training options.\\nSqueezeNet ShuffleNet MobileNet_v2 ResNet-50\\nDataset Classification Training Classification Training Classification Training Classification Training\\naccuracy (%) time (s) accuracy (%) time (s) accuracy (%) time (s) accuracy (%) time (s)\\nForest-Fire 97.11 45 97.89 68 98.95 151 97.63 166\\nFire-Flame 95.00 77 96.00 90 97.50 164 96.00 175\\nIn the case of mildly noisy images, Table 4 shows the results of classification accuracy with Gaussian and Salt\\n& Pepper noise (in the parentheses are the drops in classification accuracy compared to the values in Table 3). For\\ncomparison reasons, we have set both types of noise to have an average PSNR of approximately 15 dB.\\nTable 4. Classification accuracy per dataset and CNN for noised test images with PSNR = 15 dB.\\n(PSNR = SqueezeNet ShuffleNet MobileNet_v2 ResNet-50\\n15 dB)\\nDataset Gaussian Salt & Gaussian Salt & Gaussian Salt & Gaussian Salt &\\nPepper Pepper Pepper Pepper\\nForest-Fire 76.58 86.05 80.53 87.63 67.37 85.26 91.20 94.58\\n(−20.53) (−11.06) (−17.36) (−10.26) (−31.58) (−13.69) (−6.43) (−3.05)\\nFire-Flame 87.00 87.00 90.00 91.00 82.50 89.50 94.00 94.50\\n(−8.00) (−8.00) (−6.00) (−5.00) (−15.00) (−8.00) (−2.00) (−1.50)\\n253\\nE. Tsalera, A. Papadakis, I. Voyiatzis et al. Energy Reports 9 (2023) 247–257\\nRegarding the two noises, Gaussian noise causes more destructive results in classification accuracy than Salt &\\nPeppernoise.Concerningthetwodatasets,theForest-Firehasanaveragedecreaseofclassificationaccuracyaround\\n19% with Gaussian noise and 9.7% with the Salt & Pepper. The Fire dataset appears to be less affected with an\\naveragedecreaseofclassificationaccuracyabout7.8%and5.4%withGaussianandSalt&Peppernoiserespectively.\\nAsfortheCNNs,ResNet-50ismoretherobusttonoisewithanaveragedecreaseinclassificationaccuracyaround\\n3.2%, followed by ShuffleNet with an average decrease of 9.7%, while the most vulnerable network to appear\\nMobileNet v2 with an average decrease of 17.1%. So, taking into account the comparable results of Table 3 but\\nalso the noise tolerance, ResNet-50 and ShuffleNet seem to be the best choices.\\nTheresultsofclassificationaccuracyforhighlevelofnoise,(PSNRof10dB)areshowninTable5.Forthelight-\\nweight CNNs the classification accuracy drops rapidly and fluctuates around 50%. ResNet-50 results are slightly\\nbetter for the case of Forest-Fire dataset, while in the Fire-Flame dataset Gaussian noise has less impact upon the\\nclassification accuracy (80.8%) than Salt & Pepper (72%).\\nTable 5. Classification accuracy per dataset and CNN for noised test images with PSNR = 10 dB.\\n(PSNR = SqueezeNet ShuffleNet MobileNet_v2 ResNet-50\\n10 dB)\\nDataset Gaussian Salt & Gaussian Salt & Gaussian Salt & Gaussian Salt &\\nPepper Pepper Pepper Pepper\\nForest-Fire 50.00 51.50 50.26 53.42 50.00 50.00 63.2 54.1\\n(−47.11) (−45.61) (−47.63) (−44.47) (−48.95) (−48.95) (−34.43) (−45.53)\\nFire-Flame 49.50 55.50 57.00 72.50 49.00 49.00 80.8 72.00\\n(−8.00) (−39.5) (−6.00) (−23.50) (−15.00) (−48.50) (−15.20) (−24.00)\\nTheretrainedCNNsareevaluatedonimagesoutofthetrainingdomain(cross-dataset).The“UnknownImages”\\nset is used first with clear images and then with highly noised (i.e. setting PSNR equal to 10 dB). In this way we\\nexamine a) whether ShuffleNet remains the dominant choice among the light-weight CNNs and b) whether either\\ndataset is more suitable for training (i.e. contains more strongly differentiated images). The results are presented in\\nTable 6.\\nTable 6. Cross-dataset classification results on “Unknown Images” for clear and noised images.\\nTraining dataset Forest-Fire Fire-Flame\\nCNN Test set Unknown Images Unknown Images\\nClear Images 85.50 85.50\\nSqueezeNet Gaussian 77.00 72.00\\nSalt & Pepper 67.00 61.00\\nClear Images 90.00 90.00\\nShuffleNet Gaussian 71.50 71.00\\nSalt & Pepper 65.50 65.00\\nClear Images 86.50 86.00\\nMobileNet_v2 Gaussian 71.50 71.00\\nSalt & Pepper 65.50 65.00\\nClear Images 85.50 85.50\\nResNet-50 Gaussian 82.50 79.00\\nSalt & Pepper 73.00 71.00\\nAccordingtothevaluesofTable6,ShuffleNetyieldsthehighestclassificationperformanceforthecross-dataset\\ntest. It is also confirmed that ResNet-50 handles the effect of noises on images better. Regarding which set is more\\nsuitable for training, the results are comparable with slightly increased percentages in the case of networks trained\\non the Forest dataset.\\n254\\nE. Tsalera, A. Papadakis, I. Voyiatzis et al. Energy Reports 9 (2023) 247–257\\n5. Conclusions and future work\\nWildfires represent a significant risk factor of environment debasement and they have impact upon human lives\\nand activities. The main objective is the prevention and the early detection of fires, and to this end progress has\\nbeen made in terms of a) electro-mechanical means able to retrieve images from geographically remote settings\\n(suchasUAVs)andb)ofmachinelearning(ML)(especiallyCNN-based)meansabletoprocessfiguresandextract\\ninterestinginformation.Theseperspectivescanbecombinedandoffernearrealtimecapabilities,withtheconstraint\\nof limited computational resources.\\nTo this end we have designed a CNN-based system able to process images in order to identify fires and extract\\ncontext-basedinformationthroughsemanticsegmentations.Asthekeyconstraintisthelimitationofcomputational\\nresources, we have selected three lightweight CNNs (ShuffleNet, SqueezeNet, and MobileNet v2) and a larger\\none (ResNet-50) for comparison purposes. In principle the CNNs can achieve high accuracy even under normal\\nconditions (i.e. using images of acceptable quality, without noise), with classification accuracy of more than 95%.\\nAs a key particularity of the application of ML-based solution has been the limited availability of reusable\\ndatasets related to wildfires, we have performed a series of tests involving noisy images (at two levels of PSNR) as\\nwell as cross-dataset scenarios. For mild noise of up to 15 dB, light-weight CNNs (and especially the ShufleNet)\\nappear preferable as they achieve high classification accuracy and do not require a lot of computing resources. If\\nthe noise is of a higher level and assuming that denoising mechanisms may improve the quality of the image for\\nhuman perception but the noise still affects the ML algorithms, larger networks (in terms of number of parameters)\\nhavetobeused.WehavealsoobservedthatwhenthenoiseisatamoderateleveltheGaussiancausesaffectsmore\\nthe classification accuracy, while when the noise level increases to high levels the Salt & Pepper is proved to be\\nmore destructive. In terms of the semantic segmentation, it has been verified that objects related to infrastructures\\n(e.g powergenerating plants, electricalconnectors, posts and transformersfor the supplyof electricity andenergy),\\nwhich are of interest in this work, exist in the main object detection sets (CamVid and COCO4) and the usage of\\nCNNs trained for object detection (namely based upon ResNet-18) has yielded acceptable results.\\nAsfuturework,technologically,wewillcontinuetheexperimentationwithandevaluationofadditionalCNNsin\\ntheareaoffireidentification.Wewillalsoworkonsemanticsegmentation,withtheaimofincludingfurtherarbitrary\\nobjects,notnecessarilyincludedinpre-existingdatasets.Furthermore,theproposedsystemcanbeintegratedwitha\\nmoreholisticframeworkoffireprevention,detection,andextinctiondecisions.Suchanexampleispresentedin[33]\\nwhoproposecustomizedrisklevelsglobaldangerofafireandoveralimitedareaoftheregion.Anotherinteresting\\nperspective is related to the prediction of the ‘extreme’ events [34]. Considering that the forest wildfires are such\\nextreme events. i.e., they occur rarely and arise from seemingly normal conditions, an ambitious goal would be a\\nprediction model for possible wildfires.\\nCRediT authorship contribution statement\\nEleni Tsalera: Conceptualization, Methodology, Algorithm development, Experiment, Results analysis, Writing\\n– original draft. Andreas Papadakis: Conceptualization, Methodology, Supervision, Results analysis, Writing –\\noriginal draft, Writing – review & editing. Ioannis Voyiatzis: Validation, Writing – review & editing, Formal\\nanalysis, Supervision. Maria Samarakou: Conceptualization, Supervision, Literature review, Writing – original\\ndraft, Writing – review & editing, Validation.\\nDeclaration of competing interest\\nThe authors declare that they have no known competing financial interests or personal relationships that could\\nhave appeared to influence the work reported in this paper.\\nData availability\\nData will be made available on request.\\n4 https://cocodataset.org/\\n255\\nE. Tsalera, A. Papadakis, I. Voyiatzis et al. Energy Reports 9 (2023) 247–257\\nAcknowledgments\\nThe authors acknowledge financial support for the dissemination of this work from the Special Account for\\nResearch Grants, University of West Attica and the Special Account for Research of ASPETE, Greece through the\\nfunding program ‘Strengthening ASPETE’s research.\\nReferences\\n[1] JesusSan-Miguel-Ayanz,TracyDurrant,RobertoBoca,PieralbertoMaianti,GiorgioLiberta‘,TomasArtesVivancos,etal.,Forestfires\\nin Europe, Middle East and North Africa 2021, Publications Office of the European Union, Luxembourg, ISBN: 978-92-76-58616-6,\\n2022, 2022.\\n[2] Ahmad Alkhatib, A review on forest fire detection techniques, Int J Distrib Sens Netw 10 (3) (2014) 2014.\\n[3] ThierryBouwmans,JavedSajid,MaryamSultana,S.KiJung,Deepneuralnetworkconceptsforbackgroundsubtraction:Asystematic\\nreview and comparative evaluation, Neural Netw 117 (2019) (2019) 8–66.\\n[4] Qi-xing Zhang, Gao-hua Lin, Yong-ming Zhang, Gao Xu, Jin-jun Wang, Wildland forest fire smoke detection based on faster R-CNN\\nusing synthetic smoke images, Procedia Eng 211 (2018) (2018) 441–446.\\n[5] AminKhatami,SaeedMirghasemi,AbbasKhosravi,C.PengLim,SaeidNahavandi,AnewPSO-basedapproachtofireflamedetection\\nusing K-medoids clustering, Expert Syst Appl 68 (2017) (2017) 69–80.\\n[6] ByoungChul Ko, Jin-Hun Jung, Nam Jae-Yeal, Fire detection and 3D surface reconstruction based on stereoscopic pictures and\\nprobabilistic fuzzy logic, Fire Saf J 68 (2014) (2014) 61–70.\\n[7] ChiYuan,ZhixiangLiu,ZhangYoumin,Vision-basedforestfiredetectioninaerialimagesforfirefightingusingUAVs,in:Proceedings\\nof international conference on unmanned aircraft systems, 2016, pp. 1200–1205, 2016.\\n[8] Yi Zhao, Jiale Ma, Xiaohui Li, Jie Zhang, Saliency detection and deep learning-based wildfire identification in UAV imagery, Sensors\\n18 (3) (2018) 712, 2018.\\n[9] Eleni Tsalera, Andreas Papadakis, Maria Samarakou, Novel principal component analysis-based feature selection mechanism for\\nclassroom sound classification, Comput Intell 37 (4) (2021) 1827–1843.\\n[10] AnabelGómez-Ríos,SihamTabik,JulianLuengo,A.S.M.Shihavuddin,BartoszKrawczyk,FranciscoHerrera,Towardshighlyaccurate\\ncoral texture images classification using deep convolutional neural networks and data augmentation, Expert Syst Appl 118 (2019)\\n315–328, 2019.\\n[11] Eleni Tsalera, Andreas Papadakis, Maria Samarakou, Ioannis Voyiatzis, Feature extraction with handcrafted methods and convolutional\\nneural networks for facial emotion recognition, Appl Sci 12 (17) (2022) 8455.\\n[12] Fengju Bu, M. Samadi Gharajeh, Intelligent and vision-based fire detection systems: A survey, Image Vis Comput J 91 (2019) 2019.\\n[13] Qingjie Zhang, Jiaolong Xu, Liang Xu, Haifeng Guo, Deep convolutional neural networks for forest fire detection, in: Proceedings of\\nthe international forum on management, education and information technology application, 2016, pp. 568–575, 2016.\\n[14] A.I.MubarakMahmoud,HongeRen,Forestfiredetectionusingarule-basedimageprocessingalgorithmandtemporalvariation,Math\\nProbl Eng (2018).\\n[15] Khan Muhammad, Jamil Ahmad, Zhihan Lv, Paolo Bellavista, Po Yang, Sung Wook Baik, Efficient deep CNN-based fire detection\\nand localization in video surveillance applications, IEEE Trans Syst, Man, Cybern: Syst 49 (7) (2018) 1419–1434, 2018.\\n[16] KuldoshbayAvazov,MukhriddinMukhiddinov,FazliddinMakhmudov,YoungImCho,Firedetectionmethodinsmartcityenvironments\\nusing a deep-learning-based approach, Electronics 11 (1) (2022) 73.\\n[17] Yaowen Hu, Jialei Zhan, Guoxiong Zhou, Aibin Chen, Weiwei Cai, Kun Guo, et al., Fast forest fire smoke detection using MVMNet,\\nKnowl-Based Syst 241 (2022) 2022.\\n[18] Hao Xu, Bo Li, Fei Zhong, Light-YOLOv5: A lightweight algorithm for improved YOLOv5 in complex fire scenarios, Appl Sci 12\\n(23) (2022) 2022.\\n[19] Jianmei Zhang, Hongqing Zhu, Pengyu Wang, Xiaofeng Ling, ATT squeeze U-net: A lightweight network for forest fire detection and\\nrecognition, IEEE Access 9 (2021) (2021) 10858–10870.\\n[20] Pengyu Wang, Jianmei Zhang, Hongqing Zhu, Fire detection in video surveillance using superpixel-based region proposal and\\nESE-ShuffleNet, Multimedia Tools Appl (2021) (2021) 1–28.\\n[21] N. Forrest Iandola, Song Han, W. Moskewicz Moskewicz, Khalid Ashraf, J. William Dally, Kurt Keutzer, SqueezeNet: AlexNet-level\\naccuracy with 50x fewer parameters and <0.5 MB model size, 2016, 2016.\\n[22] NingingMa,XiangyuZhang,Hai-TaoZheng,JianSun,ShuffleNetV2:PracticalguidelinesforefficientCNNarchitecturedesign,Proc\\nEur Conf Comput Vis (2018) (2018) 116–131.\\n[23] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen, MobileNetV2: Inverted residuals and linear\\nbottlenecks, in: Proceedings of the 2018 IEEE/CVF conference on computer vision and pattern recognition, 2018, pp. 4510–4520,\\n2018.\\n[24] Kaiming He, Zhang Xizngyu, Shaoqing Ren, Jian Sun, Deep residual learning for image recognition, in: Proceedings of the 2016\\nIEEE conference on computer vision and pattern recognition, 2016, pp. 770–778, 2016.\\n[25] Alexander Kensert, J. Philip Harrison, Ola Spjuth, Transfer learning with deep convolutional neural networks for classifying cellular\\nmorphological changes, SLAS Discov: Adv Life Sci R & D 24 (4) (2019) 466–475.\\n[26] Liang-ChiehChen,YukunZhu,GeorgePapandreou,FlorianSchroff,AdamHartwig,Encoder-decoderwithatrousseparableconvolution\\nfor semantic image segmentation, in: Proceedings of the European conference on computer vision, 2018, pp. 801–818, 2018.\\n256\\nE. Tsalera, A. Papadakis, I. Voyiatzis et al. Energy Reports 9 (2023) 247–257\\n[27] J.GabrielBrostow,JulienFauqueur,RobertoCipolla,Semanticobjectclassesinvideo:Ahigh-definitiongroundtruthdatabase,Pattern\\nRecognit Lett 30 (2) (2009) 88–97.\\n[28] Ali Khan, Bilal Hassan, Somaiya Khan, Ramsha Ahmed, Abuassba Adnan, DeepFire: A novel dataset and deep transfer learning\\nbenchmark for forest fire detection, Mob Inf Syst (2022).\\n[29] Ali Ramezani-Kebrya, Ashish Khisti, Ben Liang, On the generalization of stochastic gradient descent with momentum, 2021, arXiv\\npreprint arXiv:1809.04564, 2018.\\n[30] S. Nitish Keskar, Richard Socher, Improving generalization performance by switching from adam to sgd, 2017, arXiv:1712.07628,\\n2017.\\n[31] C. Sandeep Kumain, Maheep Singh, Navjot Singh, Krishan Kumar, An efficient Gaussian noise reduction technique for noisy images\\nusing optimized filter approach, in: Proceedings of the first international conference on secure cyber computing and communication,\\nIEEE, 2018, pp. 243–248, 2018.\\n[32] Bo Fu, Xizoyang Zhao, Chuanming Song, Ximing Li, Xianghai Wang, A salt and pepper noise image denoising method based on the\\ngenerative classification, Multimedia Tools Appl 78 (2019) (2019) 12043–12053.\\n[33] Manuel Casal-Guisande, Jose-Benito Bouza-Rodríguez, Jorge Cerqueiro-Pequeño, Alberto Comesaña-Campos, Design and conceptual\\ndevelopment of a novel hybrid intelligent decision support system applied towards the prevention and early detection of forest fires,\\nForests 14 (172) (2023) 2023.\\n[34] Ethan Pickering, Stephen Guth, E. George Karniadakis, P. Themistoklis Sapsis, Discovering and forecasting extreme events via active\\nlearning in neural operators, Nature Comput Sci 2 (2022) (2022) 823–833.\\n257', 'response': 'This study explores lightweight CNNs such as SqueezeNet, ShuffleNet, and ResNet50 for wildfire identification and contextualization, achieving 96% classification accuracy.'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"train_data.json\")\n",
    "print(dataset[\"train\"][0])  # Check a sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "894305b5e5dd4bfcb9090b5c4c9d245b",
      "d0a44a0d0cb34efbaf1a24a343db25df",
      "c62a71d4c38e42a183c9643edfaf5f8c",
      "c88630d5d4304ae586087c5914de16d0",
      "9d6709b700314992a057adc1d5bdabbd",
      "f91e9d5426ff442babb9d5ec014ccef8",
      "4dd031cbf2a843c382de6424adb42a48",
      "d4d3c3fac4df40abaf9b7d15e5e073f4",
      "96c3c7f9980445fca412972aff7f8188",
      "cb55e4af8e8f4bb49c4900920caed39a",
      "3028f8f59bd046c48b9239177d5ce671"
     ]
    },
    "executionInfo": {
     "elapsed": 28794,
     "status": "ok",
     "timestamp": 1739537568956,
     "user": {
      "displayName": "Prathamjyot Singh",
      "userId": "15862000752257668268"
     },
     "user_tz": -330
    },
    "id": "Fz2WvWEZQoYl",
    "outputId": "1790eef5-a371-4d00-a90d-f0f84a05441a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "894305b5e5dd4bfcb9090b5c4c9d245b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: google/gemma-2b-it\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "model_name = \"google/gemma-2b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load model in bfloat16 precision\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Check model size\n",
    "print(f\"Model loaded: {model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739537568956,
     "user": {
      "displayName": "Prathamjyot Singh",
      "userId": "15862000752257668268"
     },
     "user_tz": -330
    },
    "id": "2wPulK7oQuIX",
    "outputId": "284de209-0418-47f6-fed8-19847dfa92df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,843,200 || all params: 2,508,015,616 || trainable%: 0.0735\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,   # Rank (controls the number of trainable parameters)\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    lora_dropout=0.1,  # Dropout rate for regularization\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"  # Causal Language Modeling (for text generation)\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "2d3d000044ad4b19be7856046f7f844c",
      "832772c910094c31b8bd93d46ebfb365",
      "bb2b5dcfed554a039656f031fdbc6690",
      "fc679c6e6f8c411694c73800e8d266df",
      "1ba7ca375def40128f1be53c1ce5e037",
      "4e7a8a70572b4f9c91f99a8fcd77be66",
      "ade1786f61e84f8d9a795793c118e2e1",
      "d27292ed63e94371929b04f1687f4002",
      "aeee11e7f21f4e8984fb250763bda397",
      "970084cfbf8c44b69b0118acb3fae0de",
      "337420c4c03c48ec8cf1d560a593d15e"
     ]
    },
    "executionInfo": {
     "elapsed": 1512,
     "status": "ok",
     "timestamp": 1739537582707,
     "user": {
      "displayName": "Prathamjyot Singh",
      "userId": "15862000752257668268"
     },
     "user_tz": -330
    },
    "id": "RcVvVvtkRYhw",
    "outputId": "008237fb-f30f-44a0-db6b-304cff41ddf8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d3d000044ad4b19be7856046f7f844c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "# ✅ Load custom dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"train_data.json\")\n",
    "\n",
    "# ✅ Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"input\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()  # Set labels for training\n",
    "    return tokenized\n",
    "\n",
    "# ✅ Apply tokenization\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 596,
     "status": "ok",
     "timestamp": 1739537587509,
     "user": {
      "displayName": "Prathamjyot Singh",
      "userId": "15862000752257668268"
     },
     "user_tz": -330
    },
    "id": "DpA88FsGRfC1",
    "outputId": "efda19fc-4606-4789-f097-2329ecf2fbe4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gemma_finetuned\",  # Save model here\n",
    "    per_device_train_batch_size=4,   # Adjust batch size based on GPU memory\n",
    "    gradient_accumulation_steps=8,   # Effective batch size = batch_size * accumulation_steps\n",
    "    num_train_epochs=3,              # Number of epochs\n",
    "    learning_rate=5e-5,               # Learning rate\n",
    "    save_total_limit=2,               # Keep only the last 2 checkpoints\n",
    "    save_strategy=\"epoch\",            # Save model at each epoch\n",
    "    evaluation_strategy=\"no\",         # No evaluation dataset\n",
    "    report_to=\"none\",                 # Disable logging (optional)\n",
    "    push_to_hub=False                 # Disable Hugging Face hub (optional)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "executionInfo": {
     "elapsed": 13116,
     "status": "ok",
     "timestamp": 1739537605039,
     "user": {
      "displayName": "Prathamjyot Singh",
      "userId": "15862000752257668268"
     },
     "user_tz": -330
    },
    "id": "870Q7lQjRh0m",
    "outputId": "04e8300c-c4df-46f2-9c4d-da4c0880e320"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-04b018290d7e>:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=13.9375, metrics={'train_runtime': 12.2132, 'train_samples_per_second': 0.246, 'train_steps_per_second': 0.246, 'total_flos': 18282033709056.0, 'train_loss': 13.9375, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ✅ Custom Trainer to compute loss\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):  # ✅ Added **kwargs\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Compute CrossEntropy loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# ✅ Initialize Trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],  # Use tokenized dataset\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# ✅ Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1236,
     "status": "ok",
     "timestamp": 1739537625029,
     "user": {
      "displayName": "Prathamjyot Singh",
      "userId": "15862000752257668268"
     },
     "user_tz": -330
    },
    "id": "OWDw_nt7RlDR",
    "outputId": "aed3bcb9-2729-44b3-9fa5-60601f7fe004"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./gemma_finetuned/tokenizer_config.json',\n",
       " './gemma_finetuned/special_tokens_map.json',\n",
       " './gemma_finetuned/tokenizer.model',\n",
       " './gemma_finetuned/added_tokens.json',\n",
       " './gemma_finetuned/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./gemma_finetuned\")\n",
    "tokenizer.save_pretrained(\"./gemma_finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "executionInfo": {
     "elapsed": 376,
     "status": "ok",
     "timestamp": 1739537113509,
     "user": {
      "displayName": "Prathamjyot Singh",
      "userId": "15862000752257668268"
     },
     "user_tz": -330
    },
    "id": "IULx6HMtUw0C"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1739537077233,
     "user": {
      "displayName": "Prathamjyot Singh",
      "userId": "15862000752257668268"
     },
     "user_tz": -330
    },
    "id": "PzG7aosoVQjw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1739537077233,
     "user": {
      "displayName": "Prathamjyot Singh",
      "userId": "15862000752257668268"
     },
     "user_tz": -330
    },
    "id": "D2EB2Z7FVQdq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "aborted",
     "timestamp": 1739537077234,
     "user": {
      "displayName": "Prathamjyot Singh",
      "userId": "15862000752257668268"
     },
     "user_tz": -330
    },
    "id": "1CdAJ-rgVQYq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "c5d837aa7ec341949d0834340b389681",
      "5c6b9d1b6f5b45df9d8992fb87abc55d",
      "fff38ecbeb7d407ba1336f1179c4eafc",
      "0d953ecbd8964afb8a2fe077a97809fb",
      "8dd6343e18b1465480533a14a83e9334",
      "c424dd965b3746b095d3a0fcd9d2730e",
      "e5ef850c54f34fbc9acd69f587e75761",
      "0ae8c13150bc4f178b283cbb73eb6f7d",
      "1793d9e3d2d64f40bae25ce3e9206175",
      "eb222cc4f908488fa6f481988b90d6ee",
      "50152b0bbce6447f8b5bc6cf0383e79d"
     ]
    },
    "executionInfo": {
     "elapsed": 29574,
     "status": "ok",
     "timestamp": 1739537660656,
     "user": {
      "displayName": "Prathamjyot Singh",
      "userId": "15862000752257668268"
     },
     "user_tz": -330
    },
    "id": "a7Falzj_VQLc",
    "outputId": "5dc943e6-c55b-4b96-849c-6e5ef15cc5b7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d837aa7ec341949d0834340b389681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Define model path\n",
    "model_path = \"./gemma_finetuned\"\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")  # Auto GPU/CPU allocation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 771,
     "status": "ok",
     "timestamp": 1739537671481,
     "user": {
      "displayName": "Prathamjyot Singh",
      "userId": "15862000752257668268"
     },
     "user_tz": -330
    },
    "id": "L-iSxnLXVRnx",
    "outputId": "9919f248-76f1-4b12-8035-88aa5a97df10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Detection of Forest Fire Using Fine-tuned \n",
      "MobileNetV2: A Lightweight Deep Learning Approach \n",
      "Sanjeev Rao1 [0000-0001-7338-1930], Prathamjyot Singh2 [0009-0000-2164-310X], Moksh Sharma3 \n",
      "[0009-0005-1574-076X] and Yugan Dhar4 [0009-0002-9051-1965] \n",
      "1,2,3,4 Computer Science and Engineering Department \n",
      "1,2,3,4 Thapar Institute of Engineering and Technology, Patiala, Punjab, India \n",
      "1 sanjeev.rao@thapar.edu, 2 psingh1_be22@thapar.edu,  \n",
      "3 msharma2_be22@thapar.edu and 4 ydhar_be22@thapar.edu \n",
      "Ab\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Path to the research paper\n",
    "pdf_path = \"/content/Final manuscript _30 Jan.pdf\"\n",
    "\n",
    "# Read and extract text from the PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text(\"text\") + \"\\n\\n\"  # Extract text from each page\n",
    "    return text\n",
    "\n",
    "# Extract text\n",
    "research_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Print first 500 characters to verify extraction\n",
    "print(research_text[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "executionInfo": {
     "elapsed": 725,
     "status": "ok",
     "timestamp": 1739537680151,
     "user": {
      "displayName": "Prathamjyot Singh",
      "userId": "15862000752257668268"
     },
     "user_tz": -330
    },
    "id": "-Aip_KkIWWaS",
    "outputId": "430d552f-2a8e-465b-dbba-6eef19c9dc2d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Early Detection of Forest Fire Using Fine-tuned \\nMobileNetV2: A Lightweight Deep Learning Approach \\nSanjeev Rao1 [0000-0001-7338-1930], Prathamjyot Singh2 [0009-0000-2164-310X], Moksh Sharma3 \\n[0009-0005-1574-076X] and Yugan Dhar4 [0009-0002-9051-1965] \\n1,2,3,4 Computer Science and Engineering Department \\n1,2,3,4 Thapar Institute of Engineering and Technology, Patiala, Punjab, India \\n1 sanjeev.rao@thapar.edu, 2 psingh1_be22@thapar.edu,  \\n3 msharma2_be22@thapar.edu and 4 ydhar_be22@thapar.edu \\nAbstract. Forest fire detection is essential for an efficient and rapid response to \\nenvironmental protection. Existing methods for wildfire detection often rely on \\ntraditional image processing techniques or shallow learning models, which strug-\\ngle with real-time performance and accuracy, particularly in remote forest envi-\\nronments with limited processing resources. This study addresses the perfor-\\nmance gap in forest fire detection methodologies by comparing multiple convo-\\nlutional neural network architectures. Our objective was to identify the most ac-\\ncurate and efficient model for real-time wildfire detection from RGB image data. \\nDeep learning models such as CNN, ResNet50, EfficientNetB0, EfficientNetB3, \\nSqueezeNet, ShuffleNet and MobileNetV2 are evaluated on a benchmark Wild-\\nfire dataset. Further, we proposed a lightweight MobileNetV2 approach fine-\\ntuned on the ImageNet dataset. The proposed fine-tuned MobileNetV2 approach \\noutperformed with an accuracy of 0.93. These findings significantly affect the \\ndevelopment of low-power, high-performing fire detection systems that could \\nbe deployed in remote forest environments with limited processing resources. \\nThe research done can be used for existing automated wildfire monitoring tech-\\nnologies, enabling better forest management and conservation strategies. \\nKeywords: Forest fire detection, Convolutional Neural Networks (CNNs), Effi-\\ncientNetB0, EfficientNetB3, SqueezeNet, ShuffleNet MobileNetV2, ResNet50 \\n1 \\nIntroduction \\nForest fires are among the most disastrous cases of natural disaster, extending hazard \\nnot only to ecosystems, biodiversity, and human communities but also as a contributory \\nfactor to various others. Climatic changes, anthropogenic interventions, and prolonged \\ndrought impact generating wildfires leading to extensive economic and environmental \\nlosses [21]. Depleting forest resources and destroying wildlife habitat are among the \\nimmediate consequences; in the long run, they aggravate global warming by contami-\\nnating the air and increasing carbon emissions. The increased incidence, duration, and \\nunpredictability of wildfires powered by the global rise in temperature and changing \\n\\n\\n2 \\nweather patterns indicate the urgent need for much more sophisticated early-notice \\ntools that can control their catastrophic impact.  \\nRecent advancements in the field of detection methodologies have transformed for-\\nest fire monitoring from manual supervision to artificial intelligence (AI)-driven recog-\\nnition powered by machine learning (ML) and deep learning (DL) algorithms, enhanc-\\ning precision, adaptability, and responsiveness [10, 12]. Conventional methods of forest \\nfire monitoring and detection-such as manual surveillance, optical sensors, and satellite \\nmonitoring-usually suffer from delayed detection, high operational costs, and limited \\nspatial coverage. However, modern systems, such as Convolutional Neural Networks \\n(CNNs) [7, 8, 9]. You Only Look Once (YOLO) object detection models [2, 11, 15] \\nand ensemble learning frameworks [21], towards scalable real-time detection, combine \\nmulti-modal data types, which include remote sensing as well as sensor networks and \\naerial imagery, significantly tackling challenges such as small fires, environmental var-\\niations, and smoke occlusion [5, 18, 20].  \\nTo detect forest fires, our study analyzes state-of-the-art DL architectures, including \\nMobileNetV2, ResNet50, EfficientNet [7], ShuffleNet and SqueezeNet [22]. Consider-\\ning the critical nature of wildfire detection, our approach leverages multiple edge com-\\nputing solutions so that AI models can be run on low-power embedded devices, drones, \\nand IoT-based sensor networks to eliminate or minimize reliance on cloud computing \\nand latency. We also focus on incorporating multi-modal data fusion, leveraging data \\nfrom thermal imaging, satellite imagery, air pollution, and weather conditions near the \\nfire to improve detection rates in difficult environments, like heavy smoke conditions, \\nlow visibility, and nighttime scenarios. These enhancements ensure high detection ac-\\ncuracy and robustness, making the system more resilient for real-world deployment in \\nhigh-risk forest regions. \\nAdditionally, once the model has learned key features from the training data, we \\nfurther employ techniques like rotation, zoom, brightness adjustment, and synthetic fire \\ninjection as data augmentation methods to increase the diversity of the dataset, thereby \\nimproving detection performance in various wildfire scenarios. Our model is pre-\\ntrained on numerous diverse large-scale datasets and fine-tuned under the low-data \\navailability, localized environmental setting. Using multi-modal sensor inputs and \\nlightweight DL models, our approach enables cost-effective, scalable, and resilient for-\\nest fire detection systems even in rural areas with high premeditation and low techno-\\nlogical availability. \\n2 \\nRelated Work \\nFire detection in computer vision presents unique challenges due to fire's non-rigid, \\ndynamic nature [1]. Huot et al. (2022) developed a wildfire spread prediction system \\nusing convolutional auto-encoders and ML models, achieving 28.4% AUC on an open-\\nsource wildfire dataset [6]. While traditional approaches often depended on motion, \\ncolor, or background subtraction in static surveillance, current systems focus on scala-\\nbility and real-time use [20]. Existing benchmark datasets, such as the Wildfire dataset, \\n\\n\\n3 \\nhave aided in creating models operating under a wide range of contexts [4]. Using light-\\nweight architectures such as MobileNetV2 and EfficientNet, designed for environments \\nwith limited resources, has shown great improvement in effectiveness and efficiency. \\nThese developments highlight the strength of DL in making systems for fire detection \\nmore sophisticated and less expensive to implement in many places to reduce the \\nchances of wildfire damage.  \\nModern advancements in DL have had a major impact on improving forest fire de-\\ntection systems. Xu et al. (2021) developed an ensemble solution uniting the YOLOv5, \\nEfficientNet, and EfficientNet architectures and trained it on a custom dataset of 10,581 \\npictures [21]. They achieved improvements in detecting an order of magnitude greater, \\nfrom 2.5% to 10.9%, and large decreases in false positives (51.3%), and handled dif-\\nferent fire types like ground, trunk, and canopy fires reasonably well. \\nSeydi et al. (2022) introduced the Fire-Net, a deep-learning architecture applied to \\nLandsat-8 satellite imagery. By applying a new compound loss function, Fire-Net \\nachieved an accuracy of 97.35% overall, indicating its capability for real-time active \\nforest fire detection [14]. Cao et al. (2024) improved the YOLOv5 model by adding a \\nglobal attention mechanism. Evaluated on the dataset with 2,876 public images, this \\nmodel improved by 4.6% mAP; it showed the added value of detecting small and ob-\\nscured fires [2]. Table 1 summarizes some recent works for forest fire detection. \\n \\nTable 1. Summary of related work for Forest Fire Detection \\nRefer-\\nences \\nWork \\nModels / Ap-\\nproach \\nDataset \\nUsed \\nResults \\nAnalysis \\n[21] \\n \\nForest Fire \\nDetection \\nSystem Us-\\ning Ensem-\\nble Learn-\\ning \\nYolov5 + Ef-\\nficientDet + \\nEfficientNet \\nCustom \\ndataset \\n(10,581 \\nimages) \\nImproved de-\\ntection per-\\nformance by \\n2.5%-10.9% \\nand reduced \\nfalse posi-\\ntives by \\n51.3%.  \\nEffective in \\nhandling dif-\\nferent fire \\ntypes (ground, \\ntrunk, can-\\nopy); \\n[6] \\nNext Day \\nWildfire \\nSpread \\nPrediction \\nsystem using \\nconvolutional \\nauto-encoders \\nand other ML \\nmodels. \\nHistorical \\nwildfire \\ndataset \\ncontaining \\n18,545 \\nevents \\nArea Under \\nCurve value \\nof 28.4% \\nOpen-source \\ndataset and \\nmethodology \\nfor wildfire \\nprediction \\n[14] \\nFire-Net: A \\nDL Frame-\\nwork \\nProposed a \\nDL frame-\\nwork, Fire-\\nNet \\nLandsat-8 \\nsatellite \\nimages \\nAccuracy: \\n97.35%  \\nUsed a novel \\nhybrid loss \\nfunction \\n \\n[17] \\nMulti-\\nmodal \\nFramework \\nfor Forest \\nFramework \\nintegrating \\nsensor and im-\\nage data using \\nNeuro-Fuzzy \\nand CNN-\\nData from \\nMon-\\ntesano \\nNatural \\nPark in \\nPortugal \\nAccuracy: \\n82% for \\nzones \\n \\nEnhances \\nearly fire de-\\ntection by in-\\ntegrating mul-\\ntiple data \\n\\n\\n4 \\n3 \\nMethodology \\nThe study utilizes the Wildfire Dataset, created further to develop DL-based techniques \\nfor forest fire recognition using a growing set of public data [4]. The dataset comprises \\n2700 aerial and ground images publicly available from government sources, Flickr and \\nUnsplash [5]. It covers a large range of environments, types of forests, and regions, \\nmaking it an effective benchmark for forest fire detection efforts. The dataset includes \\nhigh-quality images with pixel resolution averaging over 4,000 pixels, guaranteed for \\ncomprehensive machine learning endeavors. Apart from a high range of variability in \\nscale and context, the dataset is prospective-based and will be further augmented with \\nimages, videos, and other data types, as users suggest. \\nBefore applying these techniques, an experiment was conducted in which DL meth-\\nods were trained and evaluated for wildfire detection on a detect-and-report basis. The \\ndataset was partitioned into subsets for training, testing, and validation to assess the \\nfinal model with a superior methodology. Data preprocessing techniques such as value \\nrescaling and data augmentation through rotation, zoom, flips, and shifts for increased \\nmodel robustness were followed up with. As a result of the investigation, the most suit-\\nable light DL mod\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Truncate text if it's too long\n",
    "max_chars = 10000  # Adjust based on model's token limit\n",
    "research_text = research_text[:max_chars]\n",
    "\n",
    "research_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17833,
     "status": "ok",
     "timestamp": 1739538063954,
     "user": {
      "displayName": "Prathamjyot Singh",
      "userId": "15862000752257668268"
     },
     "user_tz": -330
    },
    "id": "IXtwEB2bWZkz",
    "outputId": "27b8a885-cd9e-4d36-9ffa-84f6784e3428"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Summary:\n",
      " Early Detection of Forest Fire Using Fine-tuned \n",
      "MobileNetV2: A Lightweight Deep Learning Approach \n",
      "Sanjeev Rao1 [0000-0001-7338-1930], Prathamjyot Singh2 [0009-0000-2164-310X], Moksh Sharma3 \n",
      "[0009-0005-1574-076X] and Yugan Dhar4 [0009-0002-9051-1965] \n",
      "1,2,3,4 Computer Science and Engineering Department \n",
      "1,2,3,4 Thapar Institute of Engineering and Technology, Patiala, Punjab, India \n",
      "1 sanjeev.rao@thapar.edu, 2 psingh1_be22@thapar.edu,  \n",
      "3 msharma2_be22@thapar.edu and 4 ydhar_be22@thapar.edu \n",
      "Abstract. Forest fire detection is essential for an efficient and rapid response to \n",
      "environmental protection. Existing methods for wildfire detection often rely on \n",
      "traditional image processing techniques or shallow learning models, which strug-\n",
      "gle with real-time performance and accuracy, particularly in remote forest envi-\n",
      "ronments with limited processing resources. This study addresses the perfor-\n",
      "mance gap in forest fire detection methodologies by comparing multiple convo-\n",
      "lutional neural network architectures. Our objective was to identify the most ac-\n",
      "curate and efficient model for real-time wildfire detection from RGB image data. \n",
      "Deep learning models such as CNN, ResNet50, EfficientNetB0, EfficientNetB3, \n",
      "SqueezeNet, ShuffleNet and MobileNetV2 are evaluated on a benchmark Wild-\n",
      "fire dataset. Further, we proposed a lightweight MobileNetV2 approach fine-\n",
      "tuned on the ImageNet dataset. The proposed fine-tuned MobileNetV2 approach \n",
      "outperformed with an accuracy of 0.93. These findings significantly affect the \n",
      "development of low-power, high-performing fire detection systems that could \n",
      "be deployed in remote forest environments with limited processing resources. \n",
      "The research done can be used for existing automated wildfire monitoring tech-\n",
      "nologies, enabling better forest management and conservation strategies. \n",
      "Keywords: Forest fire detection, Convolutional Neural Networks (CNNs), Effi-cientNet, MobileNetV2, Wildfire dataset.\n",
      "\n",
      "\n",
      "**Introduction**\n",
      "\n",
      "Forest fires pose a significant threat to human life, property, and the environ-\n",
      "ment. Early detection of forest fires is essential for a rapid and effective response\n",
      "to mitigate their devastating impact. Traditional image processing techniques and shallow \n",
      "learning models often struggle with real-time performance and accuracy in complex and challenging forest environments, due to factors such as limited field of view, low spatial resolution, and low computational resources.\n",
      "\n",
      "**Methods**\n",
      "\n",
      "We employed a diverse set of deep learning models to compare their performance for wildfire detection. These models included:\n",
      "\n",
      "* Convolutional Neural Networks (CNNs)\n",
      "* ResNet50\n",
      "* EfficientNetB0\n",
      "* EfficientNetB3\n",
      "* SqueezeNet\n",
      "* ShuffleNet\n",
      "* MobileNetV2\n",
      "\n",
      "The models were trained on a benchmark wildfire dataset and fine-tuned on the ImageNet dataset.\n",
      "\n",
      "**Results and Discussion**\n",
      "\n",
      "The results show that MobileNetV2 achieved the highest accuracy of 0.93 on the Wildfire dataset. This outperformed other models, including CNNs, ResNet50, EfficientNetB0, EfficientNetB3, SqueezeNet, and ShuffleNet.\n",
      "\n",
      "The improved performance of MobileNetV2 can be attributed to its:\n",
      "\n",
      "* Depth-wise filters, which enable the model to learn complex relationships between features in the images\n",
      "* Batch normalization, which helps to stabilize the learning process and improve generalization\n",
      "* Residual connections, which allow the model to learn from previous feature representations\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "The proposed fine-tuned MobileNetV2 approach significantly outperformed other deep learning models in terms of accuracy for forest fire detection. This approach could be used to develop low-power, high-performance fire detection systems that could be deployed in remote forest environments with limited processing resources. These systems could contribute to improving forest management and conservation strategies.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize text\n",
    "inputs = tokenizer(research_text, return_tensors=\"pt\", truncation=True, max_length=512).to(\"cuda\")\n",
    "\n",
    "# Generate summary\n",
    "output = model.generate(**inputs, max_length=1000, do_sample=True, temperature=0.7)\n",
    "\n",
    "# Decode and print the summary\n",
    "summary = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"\\n🔹 Summary:\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1739537077234,
     "user": {
      "displayName": "Prathamjyot Singh",
      "userId": "15862000752257668268"
     },
     "user_tz": -330
    },
    "id": "mIJZ_Zb6dlb_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3keTzc03j0jI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGDJSkYAj0gy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x1dBnXL7j0ec"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Cr3f9Jqj0b3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 10928,
     "status": "ok",
     "timestamp": 1739538269448,
     "user": {
      "displayName": "Prathamjyot Singh",
      "userId": "15862000752257668268"
     },
     "user_tz": -330
    },
    "id": "dlvzdkz5j0ZO",
    "outputId": "72a056ff-5208-4d93-ae27-546adb4f4915"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/gemma_finetuned.zip'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.make_archive(\"gemma_finetuned\", 'zip', \"./gemma_finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 431,
     "status": "ok",
     "timestamp": 1739538282193,
     "user": {
      "displayName": "Prathamjyot Singh",
      "userId": "15862000752257668268"
     },
     "user_tz": -330
    },
    "id": "Oy07sU4Fj1ey",
    "outputId": "65a30063-ad0e-4adb-d588-ce2c74f97085"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "download(\"download_a8ca77a0-d7cb-4076-9a1b-b8b190461695\", \"gemma_finetuned.zip\", 69202647)",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.download(\"gemma_finetuned.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRzkBunqj7Hy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM3ZyJc8ueK4Nw06+aveTTe",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ae8c13150bc4f178b283cbb73eb6f7d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0d953ecbd8964afb8a2fe077a97809fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb222cc4f908488fa6f481988b90d6ee",
      "placeholder": "​",
      "style": "IPY_MODEL_50152b0bbce6447f8b5bc6cf0383e79d",
      "value": " 2/2 [00:27&lt;00:00, 11.43s/it]"
     }
    },
    "13e69542c51d4499831050f69892f844": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_aaa8289439254898aafe11e3a1f62d81",
       "IPY_MODEL_26d36ee6dc0340c3832c6211cef78852",
       "IPY_MODEL_fedc50a9530e4906a2d38656b4010eb6"
      ],
      "layout": "IPY_MODEL_be0c51c2680e4e6885a218760f4e0bc8"
     }
    },
    "1793d9e3d2d64f40bae25ce3e9206175": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1ba7ca375def40128f1be53c1ce5e037": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ebcede07a384123aecd2ea624a1f949": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "26d36ee6dc0340c3832c6211cef78852": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b8f51492142940329e57a82fa7c97fe7",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1ebcede07a384123aecd2ea624a1f949",
      "value": 2
     }
    },
    "2903792780d140e9b18fbd1afa66342b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d3d000044ad4b19be7856046f7f844c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_832772c910094c31b8bd93d46ebfb365",
       "IPY_MODEL_bb2b5dcfed554a039656f031fdbc6690",
       "IPY_MODEL_fc679c6e6f8c411694c73800e8d266df"
      ],
      "layout": "IPY_MODEL_1ba7ca375def40128f1be53c1ce5e037"
     }
    },
    "3028f8f59bd046c48b9239177d5ce671": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "32989839807349b7957272279ac43991": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "337420c4c03c48ec8cf1d560a593d15e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3d7854e3b6304b68b9fcd1e0d2094d80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b85b211493b84a3ab46175c8deb554ce",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_32989839807349b7957272279ac43991",
      "value": 1
     }
    },
    "4dd031cbf2a843c382de6424adb42a48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4e7a8a70572b4f9c91f99a8fcd77be66": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "50152b0bbce6447f8b5bc6cf0383e79d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5c6b9d1b6f5b45df9d8992fb87abc55d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c424dd965b3746b095d3a0fcd9d2730e",
      "placeholder": "​",
      "style": "IPY_MODEL_e5ef850c54f34fbc9acd69f587e75761",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "6018d371ffe649c08060b285be00acfc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "832772c910094c31b8bd93d46ebfb365": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4e7a8a70572b4f9c91f99a8fcd77be66",
      "placeholder": "​",
      "style": "IPY_MODEL_ade1786f61e84f8d9a795793c118e2e1",
      "value": "Map: 100%"
     }
    },
    "84f4ce078faa498480692f99f57387bb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "894305b5e5dd4bfcb9090b5c4c9d245b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d0a44a0d0cb34efbaf1a24a343db25df",
       "IPY_MODEL_c62a71d4c38e42a183c9643edfaf5f8c",
       "IPY_MODEL_c88630d5d4304ae586087c5914de16d0"
      ],
      "layout": "IPY_MODEL_9d6709b700314992a057adc1d5bdabbd"
     }
    },
    "8dd6343e18b1465480533a14a83e9334": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90d4af48c9594c5fb4e10e66cc489d88": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "96c3c7f9980445fca412972aff7f8188": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "970084cfbf8c44b69b0118acb3fae0de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99ded231f22f4e36a949868736f43631": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9d6709b700314992a057adc1d5bdabbd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a251afdf854347f2a1d3d6f7cf80dcc5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aaa8289439254898aafe11e3a1f62d81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2903792780d140e9b18fbd1afa66342b",
      "placeholder": "​",
      "style": "IPY_MODEL_b34a5f8056c347cfb59bdb22b504d6d6",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "ade1786f61e84f8d9a795793c118e2e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aeee11e7f21f4e8984fb250763bda397": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b34a5f8056c347cfb59bdb22b504d6d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b85b211493b84a3ab46175c8deb554ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "b8f51492142940329e57a82fa7c97fe7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb2b5dcfed554a039656f031fdbc6690": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d27292ed63e94371929b04f1687f4002",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_aeee11e7f21f4e8984fb250763bda397",
      "value": 1
     }
    },
    "be0c51c2680e4e6885a218760f4e0bc8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c02f03f5a7334b6a81163e3aa1946d45": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c40eec7094af451393d672d9336d9b30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c424dd965b3746b095d3a0fcd9d2730e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5d837aa7ec341949d0834340b389681": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5c6b9d1b6f5b45df9d8992fb87abc55d",
       "IPY_MODEL_fff38ecbeb7d407ba1336f1179c4eafc",
       "IPY_MODEL_0d953ecbd8964afb8a2fe077a97809fb"
      ],
      "layout": "IPY_MODEL_8dd6343e18b1465480533a14a83e9334"
     }
    },
    "c62a71d4c38e42a183c9643edfaf5f8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d4d3c3fac4df40abaf9b7d15e5e073f4",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_96c3c7f9980445fca412972aff7f8188",
      "value": 2
     }
    },
    "c88630d5d4304ae586087c5914de16d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cb55e4af8e8f4bb49c4900920caed39a",
      "placeholder": "​",
      "style": "IPY_MODEL_3028f8f59bd046c48b9239177d5ce671",
      "value": " 2/2 [00:26&lt;00:00, 11.20s/it]"
     }
    },
    "c9eb2b6611ab4cddb4acc970fb6a574c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ce0c87fef2da4947a95355e529f42a2c",
       "IPY_MODEL_3d7854e3b6304b68b9fcd1e0d2094d80",
       "IPY_MODEL_e4c715592cb6416c9a0b319166feaf9d"
      ],
      "layout": "IPY_MODEL_a251afdf854347f2a1d3d6f7cf80dcc5"
     }
    },
    "cb55e4af8e8f4bb49c4900920caed39a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce0c87fef2da4947a95355e529f42a2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_84f4ce078faa498480692f99f57387bb",
      "placeholder": "​",
      "style": "IPY_MODEL_c40eec7094af451393d672d9336d9b30",
      "value": "Generating train split: "
     }
    },
    "d0a44a0d0cb34efbaf1a24a343db25df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f91e9d5426ff442babb9d5ec014ccef8",
      "placeholder": "​",
      "style": "IPY_MODEL_4dd031cbf2a843c382de6424adb42a48",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "d27292ed63e94371929b04f1687f4002": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d4d3c3fac4df40abaf9b7d15e5e073f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e4c715592cb6416c9a0b319166feaf9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c02f03f5a7334b6a81163e3aa1946d45",
      "placeholder": "​",
      "style": "IPY_MODEL_99ded231f22f4e36a949868736f43631",
      "value": " 1/0 [00:00&lt;00:00, 24.32 examples/s]"
     }
    },
    "e5ef850c54f34fbc9acd69f587e75761": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb222cc4f908488fa6f481988b90d6ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f91e9d5426ff442babb9d5ec014ccef8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc679c6e6f8c411694c73800e8d266df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_970084cfbf8c44b69b0118acb3fae0de",
      "placeholder": "​",
      "style": "IPY_MODEL_337420c4c03c48ec8cf1d560a593d15e",
      "value": " 1/1 [00:00&lt;00:00, 10.14 examples/s]"
     }
    },
    "fedc50a9530e4906a2d38656b4010eb6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_90d4af48c9594c5fb4e10e66cc489d88",
      "placeholder": "​",
      "style": "IPY_MODEL_6018d371ffe649c08060b285be00acfc",
      "value": " 2/2 [00:28&lt;00:00, 12.01s/it]"
     }
    },
    "fff38ecbeb7d407ba1336f1179c4eafc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ae8c13150bc4f178b283cbb73eb6f7d",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1793d9e3d2d64f40bae25ce3e9206175",
      "value": 2
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
